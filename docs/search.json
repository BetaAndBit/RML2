[
  {
    "objectID": "03_tree.html#python-snippets",
    "href": "03_tree.html#python-snippets",
    "title": "Step 3. Grow a tree",
    "section": "Python snippets",
    "text": "Python snippets\nThe most common way to train a decision tree in Python is by using the DecisionTreeClassifier class from sklearn.tree module. Scikit-learn provides a robust and user-friendly implementation of decision tree algorithms. The training process typically involves two steps: (1) create an instance of the decision tree classifier specifying any desired hyperparameters like tree’s depth, splitting criteria, and other aspects of the learning process, (2) train the decision tree model to the training data using the fit method.\n\nfrom sklearn import tree\n\nmodel_dtc = tree.DecisionTreeClassifier(max_depth=3, \n        ccp_alpha=0.0001, random_state=0)\nmodel_dtc.fit(covid_spring.drop(\"Death\", axis=1), \n        covid_spring.Death)\n\nTrained model can be plotted with the plot_tree method.\n\nplt.figure(figsize=(14,7))\n_ = tree.plot_tree(\n    model_dtc, \n    feature_names=covid_spring.columns\n)\n\n\n\n\nFigure 3: The first split in the tree is for the Age variable. Patients are divided into younger than 67 (left) and older than 67 (right). In the same manner, one can read other splits. The criteria adopted resulted in a tree with seven leaves. The leaves include information about the number of patients who reached that leaf and the proportion of each class.\n\n\nIn order to use dalex pipeline we need to turn the model into the Explainer.\n\nexplainer_dtc = dx.Explainer(\n    model=model_dtc,\n    data=covid_summer.drop(\"Death\", axis=1), \n    y=covid_summer.Death, \n    label='dtc'\n)\n\nAs before, we can calculate the performance of the model with the method model_performance. Here on the test data the performance is lower than for the previous model.\n\nperformance_cdc = explainer_dtc.model_performance(cutoff = 0.1)\nperformance_cdc.result\n\n\nLet’s add the AUC curve for the decision tree, to the curve obtained from the previous model.\n\nperformance_cdc.plot(performance_dtc, geom=\"roc\")\n\n\n\n\nFigure 4: ROC curves for the CDC and tree model."
  },
  {
    "objectID": "03_tree.html#r-snippets",
    "href": "03_tree.html#r-snippets",
    "title": "Step 3. Grow a tree",
    "section": "R snippets",
    "text": "R snippets\nThere are many libraries in R for training decision trees. The following snippets are based on the partykit (Hothorn and Zeileis 2015) library because it works for regression, classification and survival models and it also has good statistical properties and clear visualizations.\n\nHothorn, Torsten, and Achim Zeileis. 2015. “partykit: A Modular Toolkit for Recursive Partytioning in R.” Journal of Machine Learning Research 16: 3905–9.\n\nIn this package, statistical tests are used to evaluate separation for a split. In the example below, \\(\\alpha = 0.0001\\) means that nodes will be split as long as the p-value is below \\(0.0001\\) for the \\(\\chi^2\\) test for independence.\n\nTo train a tree, we use ctree function. The first argument is a formula describing which variable is the target and which are the explanatory variables. The second argument indicates the training data. The control argument specifies additional parameters such as node splitting criteria, maximum tree depth or maximum node size.\n\nlibrary(\"partykit\")\ntree <- ctree(Death ~., covid_spring, \n              control = ctree_control(alpha = 0.0001))\nplot(tree)\n\nThe explain function builds a uniform interface to query the model. Note that the predict_function is different than for CDC model, it is specific to party objects. The subsequent arguments indicate the test data for the explain count, model type and label.\n\n\n\nFigure 5: The first split in the tree is for the Age variable. Patients are divided into younger than 67 (left) and older than 67 (right). In the same manner, one can read other splits. The criteria adopted resulted in a tree with seven leaves. The leaves include information about the number of patients who reached that leaf and the proportion of each class.\n\n\n\nmodel_tree <- DALEX::explain(tree,\n           predict_function = function(m, x) \n                predict(m, x, type = \"prob\")[,2],\n           data = covid_summer,\n           y = covid_summer$Death == \"Yes\",\n           type = \"classification\", label = \"Tree\")\n\nOnce the explainer is prepared, we can check how good this model is. It looks like it is better than the CDC model both on the training and validation data.\n\n(mp_tree <- model_performance(model_tree, cutoff = 0.1))\n\n# Measures for:  classification\n# recall     : 0.8626609 \n# precision  : 0.1492205 \n# f1         : 0.2544304 \n# accuracy   : 0.8822 \n# auc        : 0.9136169\n\nAnd finally we plot both ROC curves.\n\nplot(mp_tree, mp_cdc, geom=\"roc\")\n\n\n\n\nFigure 6: ROC curves for the CDC and tree model. The tree model has on average better predictions."
  },
  {
    "objectID": "02_performance.html#python-snippets",
    "href": "02_performance.html#python-snippets",
    "title": "Step 2. Model Performance",
    "section": "Python snippets",
    "text": "Python snippets\nThere are many measures for evaluating predictive models, and they are implemented in sklearn.metrics Python module. Below we will show the most common measures calculated by default by the dalex package.\nFirst, we need an explainer with specified validation data (here covid_summer) and the corresponding response variable.\n\nexplainer_cdc = dx.Explainer(\n    model = model_cdc, \n    data  = covid_summer.drop(\"Death\", axis=1), \n    y     = covid_summer.Death, \n    model_type = \"classification\",\n    label  = \"cdc\"\n)\n\nModel exploration starts with an assessment of how good is the model. The model_performance() function calculates a set of measures for a specified type of task, here classification.\n\nperformance_cdc = explainer_cdc.model_performance(cutoff=0.1)\nperformance_cdc\n\n\nNote: The explainer knows whether the model is trained for classification or regression task, so it automatically selects the right performance measures. This can be overridden if needed.\nThe plot function draws a graphical summary of the model performance. With the geom argument, one can determine the type of chart.\n\nperformance_cdc.plot(geom = \"roc\")\nperformance_cdc.plot(geom = \"lift\")\n\n\n\n\nFigure 3: Left panel: ROC curve. Right panel: LIFT curve, one of the many graphical statistics used in summarizing the quality of scores, often used in credit risk scoring. The OX axis presents the fraction of assigned credits, and the OY axis presents the ratio of the Sensitivity of the tested model to the Sensitivity of the random model."
  },
  {
    "objectID": "02_performance.html#r-snippets",
    "href": "02_performance.html#r-snippets",
    "title": "Step 2. Model Performance",
    "section": "R snippets",
    "text": "R snippets\nThere are many measures for evaluating predictive models, and they are implemented in various R packages (i.e. ROCR, measures, mlr3measures). For simplicity, in this example we show only model performance measures implemented in the DALEX package.\nFirst, we need an explainer with specified validation data (here covid_summer) and the corresponding response variable.\n\nexplainer_cdc <-  DALEX::explain(model_cdc,\n                   predict_function = function(m, x) m(x),\n                   data  = covid_summer,\n                   y     = covid_summer$Death == \"Yes\",\n                   type  = \"classification\",\n                   label = \"CDC\")\n\nModel exploration starts with an assessment of how good is the model. The DALEX::model_performance function calculates a set of measures for a specified type of task, here classification.\n\nmp_cdc <- model_performance(explainer_cdc, cutoff = 0.1)\nmp_cdc\n\n# Measures for:  classification\n# recall     : 0.2188841 \n# precision  : 0.2602041 \n# f1         : 0.2377622 \n# accuracy   : 0.9673 \n# auc        : 0.906654\n\nNote: The default implementation of AUC is different in R and Python. Therefore, the results in the snippets for R will differ from those on the previous page.\nThe S3 generic plot function draws a graphical summary of the model performance. With the geom argument, one can determine the type of chart.\n\nplot(mp_cdc, geom = \"roc\")\nplot(mp_cdc, geom = \"lift\")\n\n\n\n\nFigure 4: Left panel: ROC curve. Right panel: LIFT curve, one of the many graphical statistics used in summarizing the quality of scores, often used in credit risk scoring. The OX axis presents the fraction of assigned credits, and the OY axis presents the ratio of the Sensitivity of the tested model to the Sensitivity of the random model."
  },
  {
    "objectID": "01_eda.html#python-snippet",
    "href": "01_eda.html#python-snippet",
    "title": "Step 1. Data Exploration (EDA)",
    "section": "Python snippet",
    "text": "Python snippet\nAll files needed to replicate the following code are available at website https://github.com/BetaAndBit/RML2. Download csv file with data and save them the working directory. Let’s start our analyses with reading data from these files.\n\ncovid_spring = pd.read_csv(\"py_covid_spring.csv\", delimiter=\";\")\ncovid_summer = pd.read_csv(\"py_covid_summer.csv\", delimiter=\";\")\n\nLet’s use the package matplotlib to draw a simple histogram for Age variable, and statsmodels to draw a mosaic plot for Diabetes variable.\n\nimport matplotlib.pyplot as plt\ncovid_spring.pivot(columns=\"Death\", values=\"Age\").plot.hist(bins=30)\nplt.show()\n\nfrom statsmodels.graphics.mosaicplot import mosaic\nmosaic(covid_spring, ['Death', 'Diabetes'])\nplt.show()\n\n\n\n\nFigure 1: Left panel: Histogram for the Age variable by survivor status. Right panel: The mosaic plot shows that there are significantly fewer people with diabetes, but among them the mortality is higher.\n\n\nA handy way to summarise tabular data in groups is the so-called ,,Table 1’’. This is a summary of the main characteristics of each variable broken down into groups defined by the variable of interest (here, binary information about Covid death). The name stems from the fact that this summary of the data is usually the first table to be shown in many medical and related scientific journals.\n\nfrom tableone import TableOne\ncolumns = ['Gender', 'Age', 'Cardiovascular.Diseases', 'Diabetes',\n           'Neurological.Diseases', 'Kidney.Diseases', 'Cancer',\n           'Hospitalization', 'Fever', 'Cough', 'Death']\ncategorical = ['Gender', 'Cardiovascular.Diseases', 'Diabetes',\n               'Neurological.Diseases', 'Kidney.Diseases', 'Cancer',\n               'Hospitalization', 'Fever', 'Cough']\ngroupby = ['Death']\n\nTableOne(covid_spring, columns=columns, \n        categorical=categorical, groupby=groupby, limit=1)   \n\n\n\n\nTabelOne that summarises analysed data\n\n\nOne of the most important rules to remember when building a predictive model is: Do not condition on future!. I.e. do not use variables that are not defined at the time the prediction needs to be made. Note that in the discussed case variables Hospitalization, Fever or Cough are not good predictors because they are not known in advance before infection. In the following lines, we remove invalid variables from both data sets.\n\nselected_vars = ['Gender', 'Age', 'Cardiovascular.Diseases', \n        'Diabetes', 'Neurological.Diseases', 'Kidney.Diseases', \n        'Cancer', 'Death'] \n# use only selected variables\ncovid_spring = covid_spring.loc[:, selected_vars]\ncovid_summer = covid_summer.loc[:, selected_vars]\n\nData exploration and cleansing often consume most of the time spent on data analysis. Here we have only touched on exploration, but even this initial analysis helped to determine that Age is an important characteristic (we will confirm this later). We will build further models only on variables from the selected_vars vector."
  },
  {
    "objectID": "01_eda.html#r-snippets",
    "href": "01_eda.html#r-snippets",
    "title": "Step 1. Data Exploration (EDA)",
    "section": "R snippets",
    "text": "R snippets\nThe R software offers hundreds of specialized solutions for exploratory data analysis. Certainly, many valuable solutions can be found in the book ,,R for Data Science’’ (Wickham and Grolemund 2017), but there are much more. Below we show just three examples.\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc.\nAll files needed to replicate the following code are available at https://github.com/BetaAndBit/RML. Download it and save in the working directory. Let’s start with reading the data.\n\ncovid_spring <- read.table(\"covid_spring.csv\", sep =\";\", header = TRUE)\ncovid_summer <- read.table(\"covid_summer.csv\", sep =\";\", header = TRUE)\n\nWe use the package to draw a simple histogram for Age, and ggmosaic to draw a mosaic plot for Diabetes. Note that the plots in the margins are graphically edited, so they look slightly different from the plots generated by these short instructions.\n\nlibrary(\"ggplot2\")\nggplot(covid_spring) + geom_histogram(aes(Age, fill = Death))\n\nlibrary(\"ggmosaic\")\nggplot(data = covid_spring) + \n     geom_mosaic(aes(x=product(Diabetes), fill = Death))\n\nA handy way to summarise tabular data in groups is the so-called ,,Table 1’’. This is a summary of the main characteristics of each variable broken down into groups defined by the variable of interest (here, binary information about Covid death).\n\n\n\nFigure 2: Left panel: Histogram for the Age variable by survivor status. Right panel: The mosaic plot shows that there are significantly fewer people with diabetes, but among them the mortality is higher.\n\n\n\nlibrary(\"tableone\")\nCreateTableOne(vars = colnames(covid_spring)[1:10],\n                         data = covid_spring, \n                         strata = \"Death\")\n\n#                                  Stratified by Death\n#                                   No            Yes     \n#  n                                 9487           513                    \n#  Gender = Male (%)                 4554 (48.0)    271 (52.8) 0.037\n#  Age (mean (SD))                  44.19 (18.32) 74.44 (13.2) <0.001     \n#  CardiovascularDiseases = Yes (%)   839 ( 8.8)    273 (53.2) <0.001     \n#  Diabetes = Yes (%)                 260 ( 2.7)     78 (15.2) <0.001     \n#  Neurological.Diseases = Yes (%)    127 ( 1.3)     57 (11.1) <0.001     \n#  Kidney.Diseases = Yes (%)          111 ( 1.2)     62 (12.1) <0.001     \n#  Cancer = Yes (%)                   158 ( 1.7)     68 (13.3) <0.001     \n#  Hospitalization = Yes (%)         2344 (24.7)    481 (93.8) <0.001     \n#  Fever = Yes (%)                   3314 (34.9)    335 (65.3) <0.001     \n#  Cough = Yes (%)                   3062 (32.3)    253 (49.3) <0.001     \n\nOne of the most important rules to remember when building a predictive model is: Do not condition on future!. I.e. do not use variables that are not defined at the time the prediction needs to be made. Note that in the discussed case variables Hospitalization, Fever or Cough are not good predictors because they are not known in advance before infection. In the following lines, we remove invalid variables from both data sets.\n\nselected_vars <- c(\"Gender\", \"Age\", \"Cardiovascular.Diseases\", \n    \"Diabetes\", \"Neurological.Diseases\", \"Kidney.Diseases\",\n    \"Cancer\", \"Death\")\n    \n# use only selected variables\ncovid_spring <- covid_spring[,selected_vars]\ncovid_summer <- covid_summer[,selected_vars]\n\nData exploration and cleansing often consume most of the time spent on data analysis. Here we have only touched on exploration, but even this initial analysis helped to determine that Age is an important characteristic (we will confirm this later)."
  },
  {
    "objectID": "00_hello.html#sars-cov-2-case-study",
    "href": "00_hello.html#sars-cov-2-case-study",
    "title": "Step 0. Hello model!",
    "section": "SARS-COV-2 case study",
    "text": "SARS-COV-2 case study\nTo demonstrate what responsible predictive modelling looks like, we used data obtained in collaboration with the Polish Institute of Hygiene in modelling mortality after the Covid infection. We realize that data on Coronavirus disease can evoke negative feelings. However, it is a good example of how predictive modelling can directly impact our society and how data analysis allows us to deal with complex, important and topical problems.\nAll the results presented in this book can be independently reproduced using the snippets and instructions presented in this book. If you do not want to retype them, then all the examples, data, and codes can be found on the https://betaandbit.github.io/RML/. Please note that the data presented at this URL is artificially generated to mirror relations in the actual data.\nThe procedure outlined here is presented for mortality modelling, but the same process can be replicated whether modelling patient survival, housing pricing, or credit scoring is concerned.\nWhen browsing through examples of predictive modelling, one may get the wrong impression that the life cycle of the model begins with the data from the internet and ends with validation on an independent dataset. However, this is an oversimplification. As you will see in a minute, we can create a model even without raw data.\nIn fact, the life cycle of a predictive model begins with a well-defined problem. In this example, we are looking for a model that assesses the risk of death after being diagnosed with Covid. We don’t want to guess who will survive and who won’t. Instead, we want to construct a score that allows us to sort patients by their individual risk. Why do we need such a model? For example, those at higher risk of death could be given higher protection, such as providing them with pulse oximeters or preferential vaccination. For this reason, in the following sections, we introduce and use model performance measures that evaluate rankings of scores, such as Area Under Curve (AUC). Pick a model evaluation measure suitable for the problem posed.\nHaving defined the problem we want to solve, we can move to the next step, which is to collect all the available information. Often the solution to the problem can be found in the literature, whether in the form of a ready-made feature prediction function, a discussion of what features are important, or sample data.\nIf there are no ready-to-use solutions and we have to collect the data ourselves, it is always worth considering where and what data to collect in order to build the model on a representative sample. The problem of data representativeness is a topic for a separate book. Incorrectly collected data will create biases that are hard to discover and even harder to fix.\nNote, that in this study, we used data on all patients reached by the sanitary inspectorate between March and August 2020. It would seem that data collected in this way would be free of bias, but we were able to detect some. In April, the pandemic spread faster among coal mine workers, who were more likely to be young men, which may have influenced the fluctuations in mortality.\nWe may think of a predictive model as a function that computes certain predictions for specific input data. Usually, such a function is built automatically based on the data. But technically, the model can be any function defined in any way."
  },
  {
    "objectID": "00_hello.html#cdc-statistics",
    "href": "00_hello.html#cdc-statistics",
    "title": "Step 0. Hello model!",
    "section": "CDC statistics",
    "text": "CDC statistics\nOur first model will be based on the statistics collected by the Centers for Disease Control and Prevention (CDC). That’s right; sometimes, we don’t need raw data to build a predictive model. We’ll start by turning a table with mortality statistics into a model.\nNote, that the risk for the reference group is not included in Figure 1. To calculate the mortality predictions we use here the baseline relative risk determined on Polish data at the beginning of Covid pandemic, which is 0.003% for the reference group. Currently it is much smaller as large fraction of population is already immunized.\n\n\n\nFigure 1: Mortality statistics as presented on the CDC website accessed on May 2021. This table shows rate ratios compared to the group 5- to 17-year-olds (selected as the reference group because it has accounted for the largest cumulative number of COVID-19 cases compared to other age groups)."
  },
  {
    "objectID": "00_hello.html#python-snippets",
    "href": "00_hello.html#python-snippets",
    "title": "Step 0. Hello model!",
    "section": "Python snippets",
    "text": "Python snippets\nIn this book we follow the convention that a model is an object that has function, which transforms the \\(n\\times p\\) input matrix with \\(p\\) variables for \\(n\\) observations into a vector of \\(n\\) predictions. Below, we define a class that calculates odds of Covid related death based on statistics from the CDC website.\n\nimport numpy as np\nimport pandas as pd\n\nclass cdc_risk:\n    def __init__(self, base_risk = 0.00003):\n        self.base_risk = base_risk\n        \n    def predict(self, x):\n        rratio = np.array([7900] * x.shape[0])\n        rratio[x.Age < 84.5] = 2800\n        rratio[x.Age < 74.5] = 1100\n        rratio[x.Age < 64.5] = 400\n        rratio[x.Age < 49.5] = 130\n        rratio[x.Age < 39.5] = 45\n        rratio[x.Age < 29.5] = 15\n        rratio[x.Age < 17.5] = 1\n        rratio[x.Age < 4.5] = 2\n        return rratio * self.base_risk\n\nmodel_cdc = cdc_risk()\nsteve = pd.DataFrame({\"Age\": [25], \"Diabetes\": [\"Yes\"]})\nmodel_cdc.predict(steve)\n## array([0.00045])\n\nPredictive models may have different structures. To work responsibly with a large number of models, a uniform standardized interface is needed. In this book, we use the abstraction implemented in the dalex package Baniecki et al. (2021). The Explainer constructor from this package creates an Explainer, i.e. a wrapper for the model that will allow you to work uniformly with objects of very different structures. The first argument is a model. It can be an object of any class but it is assumed that the objects contains the predict function as in our example. The label specifies a unique name that appear in the plots while other arguments will be introduced in next sections.\n\nBaniecki, Hubert, Wojciech Kretowicz, Piotr Piatyszek, Jakub Wisniewski, and Przemyslaw Biecek. 2021. “Dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python.” Journal of Machine Learning Research 22 (214): 1–7. http://jmlr.org/papers/v22/20-1473.html.\n\nimport dalex as dx\n\nexplainer_cdc = dx.Explainer(model_cdc, label=\"CDC\")\nexplainer_cdc.predict(steve)\n## array([0.00045])\n\nUsing the Explainer may seem like an unnecessary complication at the moment, but on the following pages, we show how it simplifies the work."
  },
  {
    "objectID": "00_hello.html#r-snippets",
    "href": "00_hello.html#r-snippets",
    "title": "Step 0. Hello model!",
    "section": "R snippets",
    "text": "R snippets\nA predictive model is a function that transforms the \\(n\\times p\\) data frame with \\(p\\) variables for \\(n\\) observations into a vector of \\(n\\) predictions. For further examples, below, we define a function that calculates odds of Covid related death based on statistics from the CDC website for different age groups.\n\ncdc_risk <- function(x, base_risk = 0.00003) {\n  rratio <- rep(7900, nrow(x))\n  rratio[which(x$Age < 84.5)] <- 2800\n  rratio[which(x$Age < 74.5)] <- 1100\n  rratio[which(x$Age < 64.5)] <- 400\n  rratio[which(x$Age < 49.5)] <- 130\n  rratio[which(x$Age < 39.5)] <- 45\n  rratio[which(x$Age < 29.5)] <- 15\n  rratio[which(x$Age < 17.5)] <- 1\n  rratio[which(x$Age < 4.5)]  <- 2\n  rratio * base_risk\n}\n\nsteve <- data.frame(Age = 25, Diabetes = \"Yes\")\ncdc_risk(steve)\n## [1] 0.00045 \n\nPredictive models may have different structures. To work responsibly with a large number of models, a uniform standardized interface is needed. In this book, we use the abstraction implemented in the DALEX package Biecek (2018).\n\nBiecek, Przemyslaw. 2018. “DALEX: Explainers for Complex Predictive Models in R.” Journal of Machine Learning Research 19 (84): 1–5. https://jmlr.org/papers/v19/18-416.html.\nThe explain function from this package creates an explainer, i.e. a wrapper for the model that will allow you to work uniformly with objects of very different structures. The first argument is a model. It can be an object of any class. The second argument is a function that calculates the vector of predictions. DALEX package can often guess which function is needed for a specific model, but in this book, we show it explicitly in order to emphasize how the wrapper works. The type argument specifies the model type and the label specifies a unique name that appear in the plots.\n\nlibrary(\"DALEX\")\nmodel_cdc <- DALEX::explain(cdc_risk,\n               predict_function = function(m, x) m(x),\n               type  = \"classification\",\n               label = \"CDC\")\npredict(model_cdc, steve)\n## [1] 0.00045\n\nUsing the explain function may seem like an unnecessary complication at the moment, but on the following pages, we show how it simplifies the work."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Hitchhiker’s Guide to Responsible Machine Learning",
    "section": "",
    "text": "Cover"
  },
  {
    "objectID": "04_forest.html#python-snippets",
    "href": "04_forest.html#python-snippets",
    "title": "Step 4. Plant a forest",
    "section": "Python snippets",
    "text": "Python snippets\nThe most common way to train a decision tree in Python is by using the RandomForestClassifier class from sklearn.ensemble module. Scikit-learn provides a straightforward implementation of Random Forest algorithms. As with other Scikit-learn models the training process typically involves two steps: (1) create an instance of the classifier specifying any desired hyperparameters like number of trees, (2) train the model to the training data using the fit method.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_rfc = RandomForestClassifier(n_estimators=100, random_state=0)\nmodel_rfc.fit(covid_spring.drop('Death', axis=1), covid_spring.Death)\n\nA trained model can be turned into a dalex explainer. Note that we set the label argument to control name of the model presented in diagnostic plots.\n\nexplainer_rfc = dx.Explainer(\n    model=model_rfc,\n    data=covid_summer.drop('Death', axis=1), \n    y=covid_summer.Death,\n    label='rfc'\n)\n\nWe can now check how good this model is. As expected, a random forest model has better performance/AUC than a single tree.\n\nperformance_rfc = explainer_rfc.model_performance(\n        model_type=\"classification\", cutoff=0.1)\nperformance_rfc.result\n\n\n\nperformance_cdc.plot([performance_rfc, performance_dtc], \n        geom=\"roc\")\n\n\n\n\nFigure 2: ROC curves for the CDC, tree and ranger model."
  },
  {
    "objectID": "04_forest.html#r-snippets",
    "href": "04_forest.html#r-snippets",
    "title": "Step 4. Plant a forest",
    "section": "R snippets",
    "text": "R snippets\nThe two most popular packages for training random forests in R are randomForest (Liaw and Wiener 2002) and ranger (Wright and Ziegler 2017). Both are easy to use, efficient and well parameterized. But here we use mlr3 toolkit for model training. It adds an additional level of abstraction, is a little more complex to use, but has additional features that will be used in the next section devoted to hyperparameters.\n\nLiaw, Andy, and Matthew Wiener. 2002. “Classification and Regression by randomForest.” R News 2 (3): 18–22.\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17.\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software. https://doi.org/10.21105/joss.01903.\nTraining a model with mlr3 (Lang et al. 2019) is performed in three steps.\n\nDefine the prediction task, an object that remembers the training data and the target, i.e. the variable that should be predicted\n\n\nlibrary(\"mlr3\")\n(covid_task <- TaskClassif$new(id = \"covid_spring\", \n        backend = covid_spring, \n        target = \"Death\",  positive = \"Yes\"))\n\n# <TaskClassif:covid_spring> (10000 x 8)\n# * Target: Death\n# * Properties: twoclass\n# * Features (7):\n#   - fct (6): Cancer, Cardiovascular.Diseases, Diabetes,\n#     Gender, Kidney.Diseases, Neurological.Diseases\n#   - int (1): Age\n\n\nSelect the family of models in which we want to look for a solution. There are a lot of algorithms to choose from, see the documentation. Set \"classif.ranger\" for the random forests models.\n\n\nlibrary(\"mlr3learners\")\nlibrary(\"ranger\")\ncovid_ranger <- lrn(\"classif.ranger\", predict_type=\"prob\", \n                num.trees=25)\n\n\nTrain the model with the method. The package uses R6 classes, so this method modifies the object in place.\n\n\ncovid_ranger$train(covid_task)\n\nA trained model can be turned into a DALEX explainer. Note that the predict_function is again slightly different. DALEX would guess this function based on the class of the model, but we point it out explicitly to make it easier to understand what is going on.\n\nmodel_ranger <- explain(covid_ranger,\n          predict_function = function(m,x)\n               predict(m, x, predict_type = \"prob\")[,1],\n          data = covid_summer,\n          y = covid_summer$Death == \"Yes\",\n          type = \"classification\", label = \"Ranger\")\n\nWe can now check how good this model is. As expected, a random forest model has better performance/AUC than a single tree.\n\n(mp_ranger <- model_performance(model_ranger))\n\n# Measures for:  classification\n# recall     : 0.04291845 \n# precision  : 0.4347826 \n# f1         : 0.078125 \n# accuracy   : 0.9764 \n# auc        : 0.9425837\n\nAnd finally we plot both ROC curves.\n\nplot(mp_ranger, mp_tree, mp_cdc, geom= \"roc\")\n\n\n\n\nFigure 3: ROC curves for the CDC, tree and ranger model."
  },
  {
    "objectID": "05_hpo.html#python-snippets",
    "href": "05_hpo.html#python-snippets",
    "title": "Step 5. Hyperparameter Optimisation",
    "section": "Python snippets",
    "text": "Python snippets\nThe most common way to perform a hyperparameter optimisation in Python is by using the RandomizedSearchCV class from module sklearn.model_selection.\nFirst, we need to specify the space of hyperparameters to search. Not all hyperparameters are worth optimizing. Let’s focus on three for the random forest algorithm: number of trees, max depth of a tree and the spliting critera.\n\nimport scipy\nfrom sklearn.model_selection import RandomizedSearchCV\nsearch_space = {\n    'n_estimators': scipy.stats.randint(50,  500),\n    'max_depth'   : scipy.stats.randint(1,  10),\n    'criterion'   : [\"gini\", \"entropy\"]\n}\n\nFor automatic hyperparameter search, it is necessary to specify: (1) search strategy (below it is the random search), (2) family of models to be tested, (3) definition of space for hyperparameters, (4) a procedure to evaluate the performance of the proposed models (below it is the AUC determined by 5-fold cross-validation), (5) a stopping criterion (below it is 10 evaluations).\n\nmodel_rfc_tuned = RandomizedSearchCV(\n    estimator=RandomForestClassifier(random_state=0), \n    param_distributions=search_space, \n    scoring=\"roc_auc\",\n    cv=5,\n    n_iter=10,\n    refit=True,\n    random_state=0\n)\n\nNow we are ready to fit parameters in this pipeline. As usual in can be done with fit() method. After the tuning the best identified hyperparameters cna be extracted from the best_params_ field.\n\nmodel_rfc_tuned.fit(covid_spring.drop('Death', axis=1), \n        covid_spring.Death)\nmodel_rfc_tuned.best_params_\n\n# {'criterion': 'gini', 'max_depth': 6, 'n_estimators': 242}\n\n\n\nMoreover, some algorithms, like random forests, are not very tunable. Still, we had to try!\nThere is, of course, no guarantee that the tuner will find better hyperparameters than the default ones. But in this example, the tuned model is better than all other models that we have considered so far. Let’s see how much. We need a dalex wrapper.\n\nexplainer_rfc_tuned = dx.Explainer(\n    model=model_rfc_tuned,\n    data=covid_summer.drop('Death', axis=1), \n    y=covid_summer.Death,\n    label='rfc_tuned'\n)\n\nWe can calculate and compare the model performance/AUC on validation data and then compare ROC curves for various models.\n\nperformance_rfc_tuned = explainer_rfc_tuned.model_performance(\n        model_type=\"classification\", cutoff=0.1)\nperformance_rfc_tuned.result\n\n\nLet’s plot all ROC curves in a single plot.\n\nperformance_cdc.plot([performance_rfc, performance_dtc, \n        performance_rfc_tuned], geom=\"roc\")\n\n\n\n\nFigure 2: ROC curves for the CDC, tree, ranger model and auto tune ranger model."
  },
  {
    "objectID": "05_hpo.html#r-snippets",
    "href": "05_hpo.html#r-snippets",
    "title": "Step 5. Hyperparameter Optimisation",
    "section": "R snippets",
    "text": "R snippets\nThe two most popular packages for training random forests in R are randomForest (Liaw and Wiener 2002) and ranger (Wright and Ziegler 2017). Both are easy to use, efficient and well parameterized. But here we use mlr3 toolkit for model training. It adds an additional level of abstraction, is a little more complex to use, but has additional features that will be used in the next section devoted to hyperparameters.\n\nLiaw, Andy, and Matthew Wiener. 2002. “Classification and Regression by randomForest.” R News 2 (3): 18–22.\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17.\n\nLang, Michel, Martin Binder, Jakob Richter, Patrick Schratz, Florian Pfisterer, Stefan Coors, Quay Au, Giuseppe Casalicchio, Lars Kotthoff, and Bernd Bischl. 2019. “mlr3: A Modern Object-Oriented Machine Learning Framework in R.” Journal of Open Source Software. https://doi.org/10.21105/joss.01903.\nTraining a model with mlr3 (Lang et al. 2019) is performed in three steps.\n\nDefine the prediction task, an object that remembers the training data and the target, i.e. the variable that should be predicted\n\n\nlibrary(\"mlr3\")\n(covid_task <- TaskClassif$new(id = \"covid_spring\", \n        backend = covid_spring, \n        target = \"Death\",  positive = \"Yes\"))\n\n# <TaskClassif:covid_spring> (10000 x 8)\n# * Target: Death\n# * Properties: twoclass\n# * Features (7):\n#   - fct (6): Cancer, Cardiovascular.Diseases, Diabetes,\n#     Gender, Kidney.Diseases, Neurological.Diseases\n#   - int (1): Age\n\n\nSelect the family of models in which we want to look for a solution. There are a lot of algorithms to choose from, see the documentation. Set \"classif.ranger\" for the random forests models.\n\n\nlibrary(\"mlr3learners\")\nlibrary(\"ranger\")\ncovid_ranger <- lrn(\"classif.ranger\", predict_type=\"prob\", \n                num.trees=25)\n\n\nTrain the model with the train() method. The mlr3 package uses R6 classes, so this method modifies the object in place.\n\n\ncovid_ranger$train(covid_task)\n\nA trained model can be turned into a DALEX explainer. Note that the predict_function is again slightly different. DALEX would guess this function based on the class of the model, but we point it out explicitly to make it easier to understand what is going on.\n\nmodel_ranger <- explain(covid_ranger,\n          predict_function = function(m,x)\n               predict(m, x, predict_type = \"prob\")[,1],\n          data = covid_summer,\n          y = covid_summer$Death == \"Yes\",\n          type = \"classification\", label = \"Ranger\")\n\nWe can now check how good this model is. As expected, a random forest model has better performance/AUC than a single tree.\n\n(mp_ranger <- model_performance(model_ranger))\n\n# Measures for:  classification\n# recall     : 0.04291845 \n# precision  : 0.4347826 \n# f1         : 0.078125 \n# accuracy   : 0.9764 \n# auc        : 0.9425837\n\nAnd finally we plot all ROC curves.\n\nplot(mp_ranger, mp_tree, mp_cdc, geom= \"roc\")\n\n\n\n\nFigure 3: ROC curves for the CDC, tree and ranger model."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "But what is it all about?",
    "section": "",
    "text": "All right, but how do you build predictive models in a responsible way? This is a common query posed by data scientists across varying experience levels. My reply usually boils down to a simple reply: test, test and test all over again.\nWithin this concise book, I delve into the process of developing predictive models with a strong focus on conducting detailed and versatile testing to ensure their reliability. The core of this approach lies in the model validation phase, which utilizes eXplainable Artificial Intelligence (XAI) methods. Through this phase, we gain a deeper understanding of the identified patterns, evaluating their stability and consistency with domain knowledge.\nThe book consists of 10 sections, along with an introduction, guiding readers through a step-by-step journey of creating and testing predictive models. Recognizing that building such models is an iterative process, we explore one complete iteration within these pages. The figure below illustrates the subsequent sections of the book, capturing our progression from data loading to training multiple models, culminating in an explanatory model analysis to identify the patterns learned by each model.\n\n\n\nFigure 1: The complete iteration of model building presented in this book. In the blue sections, we build a set of candidate models. The red sections describe techniques for analyzing and verifying these models\n\n\nYou will find here selected modern machine learning techniques and the intuition behind them. Methods are supplemented by code snippets with examples in Python and R. The process is shown through a comic book describing the adventures of two characters, Beta and Bit. The interaction of these two shows the decisions that analysts often face, whether to try a different model, try another technique for exploration or look for other data — questions like: how to compare models or how to validate them.\nModel development is responsible and challenging work but also an exciting adventure. Sometimes textbooks focus only on the technical side, losing all the fun. Here we are going to have it all.\n\nPrzemysław Biecek, Wilga, 2023"
  },
  {
    "objectID": "06_vi.html#python-snippets",
    "href": "06_vi.html#python-snippets",
    "title": "Step 6. Variable-importance",
    "section": "Python snippets",
    "text": "Python snippets\nWe use the model_parts method from the dalex package to calculate the importance of variables. The only required argument is the model to be analyzed. With additional arguments, one can also specify how the importance of variables is to be calculated, whether as a difference, ratio or without normalization.\n\nimportance_rfc = explainer_rfc.model_parts(\n        loss_function=\"1-auc\", \n        type=\"difference\", random_state=0)\nimportance_rfc\n\n\nThis technique is handy when we want to compare the importance of variables in different models. Let’s see what it looks like in our example. The plot method works for any number of models given as consecutive arguments.\n\nimportance_cdc = explainer_cdc.model_parts(\n    loss_function=\"1-auc\", type=\"difference\", random_state=0)\nimportance_dtc = explainer_dtc.model_parts(\n    loss_function=\"1-auc\", type=\"difference\", random_state=0)\nimportance_rfc_tuned = explainer_rfc_tuned.model_parts(\n    loss_function=\"1-auc\", type=\"difference\", random_state=0)\n\nimportance_cdc.plot([importance_rfc, importance_dtc, \n        importance_rfc_tuned], show=False)\n\n\n\n\nFigure 1: The importance of variables can be compared between various models, and it is usually a source of valuable information. In this plot, each bar starts at 1-AUC for the model on the original data and ends at 1-average AUC calculated on the data with the indicated variable permuted.\n\n\nFor the CDC model, the only important variable is Age. For the tree model, the three important variables are Age, Cancer, and Cardiovascular diseases, an observation consistent with Figure 3. For the ranger model and the model after tuning of hyperparameters, more variables are taken into account. However, Age is indisputably the most important variable in all models.\nThe same perturbation technique can be used to analyze the importance of groups of variables. Just use the variable_groups argument. Grouping variables can be particularly useful if the number of variables is large and groups of variables describe some common aspects. In our case we could group all diseases together.\nFor highly correlated variables, an interesting model exploration technique is triplot, summarising correlations structure via a dendrogram and also show the importance of groups of correlated variables. Still, variable importance analysis when variables are correlated must be performed with care. Check out the Aspect module in the dalex package."
  },
  {
    "objectID": "06_vi.html#r-snippets",
    "href": "06_vi.html#r-snippets",
    "title": "Step 6. Variable-importance",
    "section": "R snippets",
    "text": "R snippets\nWe use the model_parts function from the DALEX package to calculate the importance of variables. The only required argument is the model to be analyzed. With additional arguments, one can also specify how the importance of variables is to be calculated, whether as a difference, ratio or without normalization. The last line _baseline_ of the following listing corresponds to the difference in the loss function of a model calculated on data in which all variables have been permuted.\n\nmpart_ranger <- model_parts(model_ranger, type=\"difference\")\nmpart_ranger\n\n#                   variable mean_dropout_loss  label\n# 1             _full_model_      0.0000000000 Ranger\n# 2    Neurological.Diseases      0.0006254491 Ranger\n# 3                   Gender      0.0030246808 Ranger\n# 4          Kidney.Diseases      0.0048972639 Ranger\n# 5                   Cancer      0.0061278070 Ranger\n# 6                 Diabetes      0.0076210243 Ranger\n# 7  Cardiovascular.Diseases      0.0207565006 Ranger\n# 8                      Age      0.1580579207 Ranger\n# 9               _baseline_      0.4203818555 Ranger\n\nThis technique is handy when we want to compare the importance of variables in different models. Let’s see what it looks like in our example. The generic plot function works for any number of models given as consecutive arguments.\n\nmpart_cdc    <- model_parts(model_cdc)\nmpart_tree   <- model_parts(model_tree)\nmpart_ranger <- model_parts(model_ranger)\nmpart_tuned  <- model_parts(model_tuned)\n\nplot(mpart_cdc, mpart_tree, mpart_ranger, mpart_tuned,\n              show_boxplots = FALSE)\n\n\n\n\nFigure 2: The importance of variables can be compared between various models, and it is usually a source of valuable information. In this plot, each bar starts at 1-AUC for the model on the original data and ends at 1-average AUC calculated on the data with the indicated variable permuted.\n\n\nFor the CDC model, the only important variable is Age. For the tree model, the three important variables are Age, Cancer, and Cardiovascular diseases, an observation consistent with Figure 4. For the ranger model and the model after tuning of hyperparameters, more variables are taken into account. However, Age is indisputably the most important variable in all models.\nThe same perturbation technique can be used to analyze the importance of groups of variables. Just use the variable_groups argument. Grouping variables can be particularly useful if the number of variables is large and groups of variables describe some common aspects. In our case we could group all diseases together.\nFor highly correlated variables, an interesting model exploration technique is triplot, summarising correlations structure via a dendrogram and also show the importance of groups of correlated variables. Still, variable importance analysis when variables are correlated must be performed with care."
  },
  {
    "objectID": "07_pdp.html#python-snippets",
    "href": "07_pdp.html#python-snippets",
    "title": "Step 7. Partial Dependence and Accumulated Local Effects",
    "section": "Python snippets",
    "text": "Python snippets\nWe use the model_profile method from the dalex package to calculate the variable profile. The only required argument is the model to be analyzed. It is a good idea to specify names of variables for profile estimation as a second argument; otherwise, profiles are calculated for all variables, which can take some time. One can also specify the exact grid of values for calculations of profiles.\nThe average is calculated for the distribution specified in the data argument in the explainer. Here we calculate the PD profiles for the Age variable for covid_summer data.\n\nprofile_rfc = explainer_rfc.model_profile(variables=\"Age\")\n\nprofile_rfc.plot(show=False)\n\n\n\n\nFigure 1: Partial dependence profile for Age variable.\n\n\nSince we have four models it is worth comparing how they differ in terms of the model’s response to the Age variable.\n\nprofile_cdc = explainer_cdc.model_profile(variables=\"Age\")\nprofile_dtc = explainer_dtc.model_profile(variables=\"Age\")\nprofile_rfc_tuned = explainer_rfc_tuned.model_profile(\n        variables=\"Age\")\n\nprofile_cdc.plot([profile_rfc, profile_dtc, \n        profile_rfc_tuned], show=False)\n\n\n\n\nFigure 2: Each colour indicates a different model. The CDC model has a shifted sharp increase in risk of death. Models based on covid_spring data are more likely to place the dramatic increase in the risk around age 65. The tree model is too shallow to capture the ever-increasing risk in the oldest group. Despite this, the models are quite consistent about the general shape of the relationship."
  },
  {
    "objectID": "07_pdp.html#grouped-partial-dependence-profiles",
    "href": "07_pdp.html#grouped-partial-dependence-profiles",
    "title": "Step 7. Partial Dependence and Accumulated Local Effects",
    "section": "Grouped Partial Dependence profiles",
    "text": "Grouped Partial Dependence profiles\nBy default, the average is calculated for all observations. But with the argument one can specify a grouping variable. PD profiles are calculated independently for each level of this variable.\n\ngrouped_profile_rfc_tuned = explainer_rfc_tuned.model_profile(\n        variables=\"Age\", groups=\"Diabetes\")\ngrouped_profile_rfc_tuned.plot(show=False)\n\n\n\n\nFigure 3: Partial Dependence for Age in groups defined by Diabetes variable."
  },
  {
    "objectID": "07_pdp.html#r-snippets",
    "href": "07_pdp.html#r-snippets",
    "title": "Step 7. Partial Dependence and Accumulated Local Effects",
    "section": "R snippets",
    "text": "R snippets\nWe use the model_profile function from the DALEX package to calculate the variable profile. The only required argument is the model to be analyzed. It is a good idea to specify names of variables for profile estimation as a second argument; otherwise, profiles are calculated for all variables, which can take some time. One can also specify the exact grid of values for calculations of profiles.\nThe average is calculated for the distribution specified in the data argument in the explainer. Here we calculate the PD profiles for the Age variable for covid_summer data.\n\nmp_ranger <- model_profile(model_ranger, \"Age\")\n\nplot(mp_ranger)\n\n\n\n\nFigure 4: Partial dependence profile for Age variable.\n\n\nSince we have four models it is worth comparing how they differ in terms of the model’s response to the Age variable.\n\nmp_cdc    <- model_profile(model_cdc, \"Age\")\nmp_tree   <- model_profile(model_tree, \"Age\")\nmp_tuned  <- model_profile(model_tuned, \"Age\")\n\nplot(model_cdc, model_tree, mp_ranger, model_tuned)\n\n\n\n\nFigure 5: Each colour indicates a different model. The CDC model has a shifted sharp increase in risk of death. Models based on covid_spring data are more likely to place the dramatic increase in the risk around age 65. The tree model is too shallow to capture the ever-increasing risk in the oldest group. Despite this, the models are quite consistent about the general shape of the relationship."
  },
  {
    "objectID": "07_pdp.html#grouped-partial-dependence-profiles-1",
    "href": "07_pdp.html#grouped-partial-dependence-profiles-1",
    "title": "Step 7. Partial Dependence and Accumulated Local Effects",
    "section": "Grouped Partial Dependence profiles",
    "text": "Grouped Partial Dependence profiles\nBy default, the average is calculated for all observations. But with the argument groups one can specify a grouping variable. PD profiles are calculated independently for each level of this variable.\n\nmgroup_ranger <- model_profile(model_ranger, \"Age\", \n            groups = \"Diabetes\")\n\nplot(mgroup_ranger)\n\n\n\n\nFigure 6: Partial Dependence for Age in groups defined by Diabetes variable."
  },
  {
    "objectID": "07_pdp.html#clustered-partial-dependence-profiles",
    "href": "07_pdp.html#clustered-partial-dependence-profiles",
    "title": "Step 7. Partial Dependence and Accumulated Local Effects",
    "section": "Clustered Partial Dependence profiles",
    "text": "Clustered Partial Dependence profiles\nIf the model is additive, then individual profiles (see the next Section related to profiles) are parallel. But if the model has interactions, individual profiles may have different shapes for different values of variables in each interaction. To see if there are such interactions we can cluster the individual profiles.\nIf we specify the argument k, then the function model_profile performs a hierarchical clustering of the profiles, determines the group of k most different profiles and then calculates the Partial Dependence for each of these groups separately.\n\nmclust_ranger <- model_profile(model_ranger, \"Age\", \n            k = 3, center = TRUE)\n\nplot(mclust_ranger)\n\n\n\n\nFigure 7: Partial Dependence for three segments."
  },
  {
    "objectID": "08_shap.html#python-snippets",
    "href": "08_shap.html#python-snippets",
    "title": "Step 8. Shapley values and the Break-down plots",
    "section": "Python snippets",
    "text": "Python snippets\nLet’s define an observation for which we will examine the model more closely. Let it be a 76-year-old man with hypertension. We show a local model analysis using model_ranger as an example.\n\nSteve = pd.DataFrame({\n    \"Gender\": [1], \n    \"Age\": [76], \n    \"Cardiovascular.Diseases\": [1], \n    \"Diabetes\": [0],\n    \"Neurological.Diseases\": [0],\n    \"Kidney.Diseases\": [0],\n    \"Cancer\": [0]\n})\n\nmodel_rfc_tuned.predict_proba(Steve)\n# array([[0.65611756, 0.34388244]])\n\nThe predict_parts function for a specified model and a specified observation calculates local variable attributions. The optional argument order forces use of a specified sequence of variables. If not specified, then a greedy heuristic is used to start conditioning with the most relevant variables. The results are presented in Figure 4.\n\nbreakdown_steve = explainer_rfc_tuned.predict_parts(Steve, \n        type=\"break_down\", random_state=0)\nshap_steve = explainer_rfc_tuned.predict_parts(Steve, \n        type=\"shap\", random_state=0)\n\nbreakdown_steve.plot(show=False)\n\n\n\n\nFigure 2: Break-down contributions of each variable to the final model response.\n\n\nThe alternative is to average over all (or at least many random) orderings of variables. This is how the Shapley values are calculated. The show_boxplots argument highlights the stability of the estimated attributions between different orderings. See Figure 3.\n\nshap_steve.plot(show=False)\n\n\n\n\nFigure 3: Shapley values contributions to the final model prediction.\n\n\nThe Shapley values are additive. For models with interactions, it is often too much of simplification. Other possible values of the type argument are shap, break_down, break_down_interactions or oscillations.\nNote that by default, functions such as model_parts, predict_parts, model_profiles do not calculate statistics on the entire data set (this may be time-consuming), but on n_samples of random cases, and the entire procedure is repeated B times to estimate the error bars."
  },
  {
    "objectID": "08_shap.html#r",
    "href": "08_shap.html#r",
    "title": "Step 8. Shapley values and the Break-down plots",
    "section": "R",
    "text": "R\nLet’s define an observation for which we will examine the model more closely. Let it be a 76-year-old man with hypertension. We show a local model analysis using model_ranger as an example.\n\nSteve <- data.frame(Gender = factor(\"Male\", c(\"Female\", \"Male\")),\n   Age                     = 76,\n   Cardiovascular.Diseases = factor(\"Yes\", c(\"No\", \"Yes\")), \n   Diabetes                = factor(\"No\", c(\"No\", \"Yes\")), \n   Neurological.Diseases   = factor(\"No\", c(\"No\", \"Yes\")), \n   Kidney.Diseases         = factor(\"No\", c(\"No\", \"Yes\")), \n   Cancer                  = factor(\"No\", c(\"No\", \"Yes\")))\npredict(model_ranger, Steve)\n# 0.322\n\nThe predict_parts function for a specified model and a specified observation calculates local variable attributions. The optional argument order forces use of a specified sequence of variables. If not specified, then a greedy heuristic is used to start conditioning with the most relevant variables. The results are presented in Figure Figure 3.\n\nbd_ranger <- predict_parts(model_ranger, Steve)\nbd_ranger\n#                                       contribution\n# Ranger: intercept                         0.043\n# Ranger: Age = 76                          0.181\n# Ranger: Cardiovascular.Diseases = Yes     0.069\n# Ranger: Gender = Male                     0.033\n# Ranger: Kidney.Diseases = No             -0.004\n# Ranger: Cancer = No                      -0.002\n# Ranger: Diabetes = No                     0.003\n# Ranger: Neurological.Diseases = No        0.000\n# Ranger: prediction                        0.322\n\nplot(bd_ranger)\n\nThe alternative is to average over all (or at least many random) orderings of variables. This is how the Shapley values are calculated. The show_boxplots argument highlights the stability of the estimated attributions between different orderings. See Figure 3.\n\nshap_ranger <- predict_parts(model_ranger, Steve, type = \"shap\")\nplot(shap_ranger, show_boxplots = TRUE)\n\n\n\n\nFigure 3: Shapley values (left) and Break-down (right) illustrate the contributions of each variable to the final model response. Both attribution techniques ensure that the sum of the individual attributions adds up to the final model prediction.\n\n\nThe Shapley values are additive. For models with interactions, it is often too much of simplification. Other possible values of the type argument are shap, break_down, break_down_interactions (this option can identify pairwise interactions, see Chapter 7) or oscillations.\nNote that by default, functions such as model_parts, predict_parts, model_profiles do not calculate statistics on the entire data set (this may be time-consuming), but on n_samples of random cases, and the entire procedure is repeated B times to estimate the error bars."
  },
  {
    "objectID": "08_shap.html#r-snippets",
    "href": "08_shap.html#r-snippets",
    "title": "Step 8. Shapley values and the Break-down plots",
    "section": "R snippets",
    "text": "R snippets\nLet’s define an observation for which we will examine the model more closely. Let it be a 76-year-old man with hypertension. We show a local model analysis using model_ranger as an example.\n\nSteve <- data.frame(Gender = factor(\"Male\", c(\"Female\", \"Male\")),\n   Age                     = 76,\n   Cardiovascular.Diseases = factor(\"Yes\", c(\"No\", \"Yes\")), \n   Diabetes                = factor(\"No\", c(\"No\", \"Yes\")), \n   Neurological.Diseases   = factor(\"No\", c(\"No\", \"Yes\")), \n   Kidney.Diseases         = factor(\"No\", c(\"No\", \"Yes\")), \n   Cancer                  = factor(\"No\", c(\"No\", \"Yes\")))\npredict(model_ranger, Steve)\n# 0.322\n\nThe predict_parts function for a specified model and a specified observation calculates local variable attributions. The optional argument order forces use of a specified sequence of variables. If not specified, then a greedy heuristic is used to start conditioning with the most relevant variables. The results are presented in Figure Figure 4.\n\nbd_ranger <- predict_parts(model_ranger, Steve)\nbd_ranger\n#                                       contribution\n# Ranger: intercept                         0.043\n# Ranger: Age = 76                          0.181\n# Ranger: Cardiovascular.Diseases = Yes     0.069\n# Ranger: Gender = Male                     0.033\n# Ranger: Kidney.Diseases = No             -0.004\n# Ranger: Cancer = No                      -0.002\n# Ranger: Diabetes = No                     0.003\n# Ranger: Neurological.Diseases = No        0.000\n# Ranger: prediction                        0.322\n\nplot(bd_ranger)\n\nThe alternative is to average over all (or at least many random) orderings of variables. This is how the Shapley values are calculated. The show_boxplots argument highlights the stability of the estimated attributions between different orderings. See Figure 4.\n\nshap_ranger <- predict_parts(model_ranger, Steve, type = \"shap\")\nplot(shap_ranger, show_boxplots = TRUE)\n\n\n\n\nFigure 4: Shapley values (left) and Break-down (right) illustrate the contributions of each variable to the final model response. Both attribution techniques ensure that the sum of the individual attributions adds up to the final model prediction.\n\n\nThe Shapley values are additive. For models with interactions, it is often too much of simplification. Other possible values of the type argument are shap, break_down, break_down_interactions (this option can identify pairwise interactions, see Chapter 7) or oscillations.\nNote that by default, functions such as model_parts, predict_parts, model_profiles do not calculate statistics on the entire data set (this may be time-consuming), but on n_samples of random cases, and the entire procedure is repeated B times to estimate the error bars."
  },
  {
    "objectID": "09_cp.html#python-snippets",
    "href": "09_cp.html#python-snippets",
    "title": "Step 9. Ceteris Paribus",
    "section": "Python snippets",
    "text": "Python snippets\nThe predict_profiles() function calculates Ceteris Paribus profiles for a selected model and selected observations. By default, it calculates profiles for all variables, but one can limit this list with the variables vector of variables.\n\ncp_steve = explainer_rfc_tuned.predict_profile(Steve)\n\nThe calculated profiles can be drawn with the generic plot function. As with other explanations in the DALEX library, multiple models can be plotted on a single graph. Although for technical reasons quantitative and qualitative variables cannot be shown in a single chart. So if you want to show the importance of quality variables, you need to plot them separately.\nFigure 3 shows an example of a CP profile for continuous variable Age and categorical variable Cardiovascular.Diseases.\n\ncp_steve.plot(variables=\"Age\", show=False)\n\n\n\n\nFigure 1: The dot shows the observation under analysis. CP profile shows how the model predictions change for changes in the selected variable. On the left is the CP profile for the continuous variable Age, on the right for the categorical variable Cardiovascular.Diseases. For categorical variables, one can specify how the CP profiles should be drawn by setting the categorical_type argument.\n\n\nThe plot function can combine multiple models, making it easier to see similarities and differences.\n\ncp_cdc = explainer_cdc.predict_profile(Steve)\ncp_dtc = explainer_dtc.predict_profile(Steve)\ncp_rfc = explainer_rfc.predict_profile(Steve)\n\ncp_cdc.plot([cp_rfc, cp_dtc, cp_steve], \n        variables=\"Age\", show=False)\n\n\n\n\nFigure 2: CP profiles for Steve, colors code four considered models."
  },
  {
    "objectID": "09_cp.html#r-snippets",
    "href": "09_cp.html#r-snippets",
    "title": "Step 9. Ceteris Paribus",
    "section": "R snippets",
    "text": "R snippets\nThe predict_profiles() function calculates Ceteris Paribus profiles for a selected model and selected observations. By default, it calculates profiles for all variables, but one can limit this list with the variables vector of variables.\n\ncp_ranger <- predict_profile(model_ranger, Steve)\ncp_ranger\n#  Top profiles    : \n#       Gender   Age Cardiovascular.Diseases Diabetes\n# 1     Female 76.00                     Yes       No\n# 1.1     Male 76.00                     Yes       No\n# 11      Male  0.00                     Yes       No\n# 1.110   Male  0.99                     Yes       No\n\nThe calculated profiles can be drawn with the generic plot function. As with other explanations in the DALEX library, multiple models can be plotted on a single graph. Although for technical reasons quantitative and qualitative variables cannot be shown in a single chart. So if you want to show the importance of quality variables, you need to plot them separately.\nFigure 3 shows an example of a CP profile for continuous variable Age and categorical variable Cardiovascular.Diseases.\n\nplot(cp_ranger, variables = \"Age\")\nplot(cp_ranger, variables = \"Cardiovascular.Diseases\", \n        categorical_type = \"lines\")\n\n\n\n\nFigure 3: The dot shows the observation under analysis. CP profile shows how the model predictions change for changes in the selected variable. On the left is the CP profile for the continuous variable Age, on the right for the categorical variable Cardiovascular.Diseases. For categorical variables, one can specify how the CP profiles should be drawn by setting the categorical_type argument.\n\n\nThe plot function can combine multiple models, making it easier to see similarities and differences.\n\ncp_cdc <- predict_profile(model_cdc, Steve)\ncp_tree <- predict_profile(model_tree, Steve)\ncp_tune <- predict_profile(model_tuned, Steve)\n\nplot(cp_cdc, cp_tree, cp_ranger, cp_tune, variables = \"Age\")\n\n\n\n\nFigure 4: CP profiles for Steve, colors code four considered models.\n\n\n\n\nThe size of the oscillation can be measured in many ways, by default, it is an area between the CP profile and a horizontal line at the level of the model prediction.\nCP profiles are also useful for finding the importance of variables in a model. The more the profiles fluctuate, the more influential the variable is. Such a measure of importance is implemented in the predict_parts function under option type = \"oscillations\".\n\npredict_parts(model_ranger, Steve, type = \"oscillations\")\n#                   _vname_ _ids_ oscillations\n# 2                     Age     1   0.22872998\n# 6         Kidney.Diseases     1   0.16371903\n# 7                  Cancer     1   0.09641507\n# 4                Diabetes     1   0.05052652\n# 3 Cardiovascular.Diseases     1   0.03984208\n# 1                  Gender     1   0.03308303\n# 5   Neurological.Diseases     1   0.03164090"
  },
  {
    "objectID": "10_deployment.html#python-snippets",
    "href": "10_deployment.html#python-snippets",
    "title": "Step 10. Model Deployment",
    "section": "Python snippets",
    "text": "Python snippets\nIf we want to automate comparison of several models, is a very convenient tool for such exploration. It can work in two modes: live (with the server which adds the necessary statistics on the fly) or pre-calculated statistics. In the case of many models and large datasets, the live mode is much more convenient.\nThe snippet below turns four covid models into a dashboard.\n\narena = dx.Arena()\narena.push_model(explainer_cdc)\narena.push_model(explainer_dtc)\narena.push_model(explainer_rfc)\narena.push_model(explainer_rfc_tuned)\narena.push_observations(Steve)\n\narena.run_server()\n\nAn example dashboard built for a model for dozens of variables and several thousand rows on football player worth prediction based on the FIFA dataset is available at https://arena.drwhy.ai/?demo=1.\n\n\n\nFigure 1: The Arena web application that facilitates exploration of multiple models."
  },
  {
    "objectID": "10_deployment.html#r-snippets",
    "href": "10_deployment.html#r-snippets",
    "title": "Step 10. Model Deployment",
    "section": "R snippets",
    "text": "R snippets\nIn R one of such tools is modelStudio (Baniecki and Biecek 2020). It is a package that transforms an explainer into an HTML page with javascript based interaction. Such an HTML page is easy to save on a disk or share by email. The webpage has various explanations pre-calculated, so its generation may be time-consuming, but the model exploration is very fast, and the feedback loop is tight.\n\nBaniecki, Hubert, and Przemyslaw Biecek. 2020. “The Grammar of Interactive Explanatory Model Analysis.” Arxiv. https://arxiv.org/abs/2005.00497.\nGenerating a modelStudio for an explainer is trivially easy.\n\nlibrary(\"modelStudio\")\nms <- modelStudio(model_ranger)\nms\n\nAn example dashboard built for a model for dozens of variables and several thousand rows on football player worth prediction based on the FIFA dataset is available at https://pbiecek.github.io/explainFIFA20/.\n\n\n\nFigure 2: modelStudio is an application that facilitates model exploration using a serverless site based on javascript. The user can configure the content of each panel to look at the model from different perspectives.\n\n\nThe dashboard is created with the create_arena function. Then with push_model and push_observations, one can add more models and more observations for model exploration. The resulting object can be turned into the live web application with the run_server function.\nThe snippet below turns four covid models into a dashboard.\n\nlibrary(\"arenar\")\nlibrary(\"dplyr\")\ncovid_ar <- create_arena(live = TRUE) %>%\n    push_model(model_cdc) %>%\n    push_model(model_tree) %>%\n    push_model(model_ranger) %>%\n    push_model(model_tuned) %>%\n    push_observations(Steve) \nrun_server(covid_ar)"
  },
  {
    "objectID": "11_summary.html#other-books-from-beta-and-bit-series",
    "href": "11_summary.html#other-books-from-beta-and-bit-series",
    "title": "About this book",
    "section": "Other books from Beta and Bit series",
    "text": "Other books from Beta and Bit series\n\n\n\nChaos Game - Are you curious about fractals? The Chaos Game is the book for you. You will learn the mathematical basis behind these figures, find out what algorithm can be used to code them, write code in your favourite programming language (Python, R, Julia?) and also explore the bibliographies of three mathematicians associated with the development of mathematics around these shapes. This is the next book in the Beta Bit series for anyone interested in computational mathematics and data analysis.\nFind more at https://chaosgame.drwhy.ai/.\n\n\n\n\nChart Runners - How to create good charts? Good ones, that is, ones that are a pleasure to look at, from which a lot of information can be learned, that can be understood by a wide audience, and that savvy readers will appreciate. Based on our experience of teaching DataVis classes, Chart Runners was created. This book is a collection of short lectures discussing various threads that are useful in better understanding how communication with statistical charts works. %In the pages that follow, there will be many analogies to food preparation, because both in the kitchen and in the preparation of statistical charts one needs practice, knowledge of certain fundamental laws, a handful of tried-and-true recipes and a lot of enthusiasm for experimentation.\nFind more at https://betaandbit.github.io/Charts/."
  }
]